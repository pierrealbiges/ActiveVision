{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2017-12-11 - Apprendre le machine learning\n",
    "---\n",
    "Cet après-midi 14h : rencontre avec Daucé (modèle + machine learning + tensorflow)\n",
    "\n",
    "Butko,2010 émet deux points importants, qui ne sont pas pris en compte dans son modèle : l'incertitude (à propos de l'environnement, de ses propres mouvements et de sa perception) et l'existance de plusieurs types de mouvements oculaires (pas seulement les saccades).\n",
    "\n",
    "TensorFlow est une libraire (majoritairement python, d'autres *API* sont disponibles : Java, C++ et Go) opensource développée par Google offrant une interface pour modéliser des réseaux de neurones artificiels et réaliser du *machine learning*.  \n",
    "Leur [site officiel](https://www.tensorflow.org/) montre qu'elle est utilisée par de nombreuses entreprises et contient une grande quantité de [documentation](https://www.tensorflow.org/get_started/).  \n",
    "L'ensemble du projet est présent sur leur [GitHub](https://github.com/tensorflow/) et contient même un [site interactif](http://playground.tensorflow.org) permettant de prendre en main son fonctionnement.  \n",
    "\n",
    "Suite de **ressources à maitriser** pour bosser sur le projet :\n",
    "+ [Cours CNAM machine learning/deep learning](http://cedric.cnam.fr/vertigo/Cours/ml2/preambule.html)\n",
    "+ [Cours Coursera machine learning](https://www.coursera.org/learn/machine-learning?action=enroll)\n",
    "+ [Cours Google deep learning](https://www.udacity.com/course/deep-learning--ud730)\n",
    "\n",
    "J'ai commencé à bosser la [première ressource](http://cedric.cnam.fr/vertigo/Cours/ml2/preambule.html) et je suis presque arrivé à la fin de la première partie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2017-12-12 - Cours CNAM\n",
    "---\n",
    "\n",
    "## Introduction à l'apprentissage supervisé\n",
    "\n",
    "Pour évaluer la puissance d'un modèle, notamment son risque : méthode de l'**échantillon-test**. Division de la base d'apprentissage (D) en deux ensembles mutuellements exclusifs par sélection aléatoire (ex. D(100) = A(70)+V(30)). Réduit la taille de la base d'apprentissage (A) mais permet de calculer le risque espéré du modèle sur V (données n'ayant pas servi à l'apprentissage).  \n",
    "\n",
    "Plus robuste, la méthode de **validation croisée** propose de moyenner les résultats obtenus par plusieurs découpages différents de D.  \n",
    "Il existe plusieurs méthodes pour découper D (*Leave P out*, *Leave one out*, *k-fold*, *suffle and split*).  \n",
    "\n",
    "Evaluer la puissance d'un modèle passe aussi par la construction d'une **courbe ROC** permettant de montrer le taux de **vrais positifs**. Plus l'aire sous la courbe (*AUC*) est importante, plus le modèle est intéressant. \n",
    "\n",
    "---\n",
    "Plus d'informations sur le [mountain car problem](https://en.wikipedia.org/wiki/Mountain_car_problem) : problème communément appliqué aux modèles de *reinforcement learning* pour observer leur comportement et leur puissance.   \n",
    "L'agent comprends deux variables (position et vélocité) et trois actions (vers la gauche, neutre, vers la droite) et doit apprendre à les utiliser pour remonter une pente (qu'il ne peut pas simplement grimper, la puissance du moteur n'est pas assez importante) lui permettant d'obtenir une récompense (conséquence positive). Classiquement, chaque temps passé sans réaliser l'objectif (obtenir la récompense) est récompensé négativement (conséquence négative).  \n",
    "[Exemple d'application](https://github.com/vikasjiitk/Deep-RL-Mountain-Car)  \n",
    "\n",
    "Plus d'informations sur le [*whitening*](http://ufldl.stanford.edu/wiki/index.php/Whitening) : méthode de pré-traitement des données permettant de réduire leur dimension (en les rendant moins redondantes).  \n",
    "\n",
    "Il peut être intéressant de jeter un oeil à [ce livre](http://www.deeplearningbook.org/) disponible entièrement sur le net et écrit par des *deep-learning researchers* du MIT.\n",
    "\n",
    "---\n",
    "\n",
    "## Arbres de décision\n",
    "\n",
    "Les arbres de décision sont des graphes utilisant une représentation hiérarchique des données et comprenant trois types de noeuds :\n",
    "+ **Noeud racine** (d'où se fait l'accès à l'arbre)\n",
    "+ **Noeuds internes** (noeuds ayant des descendants et réalisant les **tests**)\n",
    "+ **Noeuds terminaux** (ou feuilles; qui n'ont pas de descendants et représentent les **décisions**)  \n",
    "\n",
    "Un élément de la base d’apprentissage situé dans un noeud ne pourra se retrouvera que dans un seul de ses descendants.  \n",
    "\n",
    "Au départ, les points des la base d’apprentissage sont tous placés dans le noeud racine.  \n",
    "\n",
    "Une des variables de description des points est la classe du point, dite « **variable cible** » (catégorielle dans un problème de classification, réelle dans un problème de régression).  \n",
    "\n",
    "Le processus s’arrête quand les éléments d’un noeud ont la même valeur pour la variable cible (homogénéité).\n",
    "\n",
    "Plusieurs types d'implantation : \n",
    "+ Iterative Dichotomiser 3 (ID3) et ses extensions ID4, ID5\n",
    "+ CART\n",
    "+ Bagging decision trees\n",
    "+ Forêts d'arbres décisionnels\n",
    "\n",
    "## Support vector machine (SVM)\n",
    "\n",
    "Base d'apprentissage {(xi,yi)} (i=1,...,n) avec :  \n",
    "\n",
    "    xi ∈ X  \n",
    "    yi ∈ {−1,+1}  \n",
    "\n",
    "Construction d'une fonction f:X→R tel que si f(x)>0 alors x est affecté à la classe +1 et si f(x)<0 alors x est affecté à la classe -1.   \n",
    "La ligne séparatrice (ou **hyperplan**) sera alors :\n",
    "\n",
    "    f(x)=0.  \n",
    "\n",
    "Pour juger la qualité d’un hyperplan en tant que séparateur on utilise la distance (marge) entre les exemples d’apprentissage et ce séparateur.   \n",
    "Les SVM linéaires cherchent l'hyperplan qui maximise la **marge** :\n",
    "\n",
    "    f(x) = ⟨w,x⟩+b = w(⊤)x+b\n",
    "\n",
    "    Si yi=1 alors w⋅xi+b≥1 et donc yi(w⋅xi+b)≥1\n",
    "    Si yi=−1 alors w⋅xi+b≤−1 et donc yi(w⋅xi+b)≥1\n",
    "  \n",
    "    Marge = 2/||w||\n",
    "    \n",
    "Souvent il arrive que les deux classes de données se retrouvent mélangées autours de l’hyperplan de séparation (donc non séparables linéairements). Pour gérer ce type de problème, on utilise une technique dite de **marge souple**, qui tolère les mauvais classements (compromis).  \n",
    "Dans cette technique, on introduit des **variables de relachement des contraintes ξi** (Absence d'erreur  ξi=0; Présence d'erreur ξi>0) qui permettent de modéliser les erreur potentielles (variables du mauvais côté de l'hyperplan) et de les pénaliser.  \n",
    "\n",
    "On introduit une **variable d’équilibrage C** (>0) qui permet d’avoir une seule fonction objectif :\n",
    "\n",
    "    min(w,b)(1/2*||w||²)+C*∑ξi\n",
    "    \n",
    "    \n",
    "Librairies majeures : **LibLinear**, **LibSVM**, **SVM-toy** et **Scikit-learn**\n",
    "L’intérêt de LibLinear est la vitesse, permettant de l’appliquer sur des grands jeux de données, mais ne peut l'être qu'à des problèmes linéaires.  \n",
    "LibSVM semble être plus puissant LibLinear (précision plus importante de la prédiction).  \n",
    "SVM-toi permet la visualisation de la classification pour des problèmes en deux dimensions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2017-12-13 - Cours CNAM (suite)\n",
    "---\n",
    "## Méthodes à noyaux\n",
    "nb. *Nombreuses formules, disponibles sur le [site](http://cedric.cnam.fr/vertigo/Cours/ml2/coursMethodesNoyaux.html).*\n",
    "\n",
    "Méthode permettant de passer d'un algorithme linéaire à une version non-linéaire, grâce à sa projection dans un espace de dimension plus grande.  \n",
    "La projection dans un espace de dimension plus grand permet de transposer les données dans un autre espace dans lequel elles devraient être **linéairement séparables** et d'y appliquer l'algorithme SVM.\n",
    "Cette projection est réalisée par des **fonctions noyaux** qui calculent un [produit scalaire](https://fr.wikipedia.org/wiki/Produit_scalaire).  \n",
    "Une fonction noyau ressemble à une **mesure de similarité** (rends une valeur importante sur les vecteurs sont similaires et une valeur faible si les vecteurs sont très différents).  \n",
    "\n",
    "Plusieurs méthodes (se basant sur le **théorème de Mercer**) permettant de construire plusieurs types de noyaux, nécessairement associés à une **matrice de Gramm** :\n",
    "+ **Noyau positif défini**\n",
    "+ **Noyau conditionnellement positif défini**\n",
    "+ **Noyau d'appariement intermédiaire**\n",
    "\n",
    "Plusieurs approches pour obtenir des fonctions noyaux :\n",
    "+ Construction directe en utilisant la projection Φ \n",
    "+ Transformation des noyaux existants (ex. l'exponentielle d'un noyau est un noyau)\n",
    "+ Combinaison des noyaux existants\n",
    "\n",
    "Pour l'estimation d'une densité de variables et de la présence ou non d'un nouveau point dans l'ensemble, il existe deux grandes approches : \n",
    "+ **SVDD** (Support Vector Data Description) qui cherche la sphère minimale englobant tous les points d'apprentissage, moins les *outliers*\n",
    "+ **OCSVM** (One Class SVM) qui cherche l'hyperplan le plus éloigné de l'origine qui contient tous les points d'apprentissages moins les *outliers*\n",
    "\n",
    "## Deep-learning\n",
    "\n",
    "Les méthodes de deep-learning (DL) répondent à un besoin : la **surabondance de données** disponibles (ex. Facebook stock 350 milliard d'images, +1 milliard/jour), nécessitant des techniques de tri et de reconnaisance automatique pour fouiller ces données et les utiliser.  \n",
    "\n",
    "Deux fonctions principales : \n",
    "+ **Classification** : assigner une donnée à une classe pré-déterminée\n",
    "+ **Reconnaissance** : mécanisme plus général que la classification (ex. localisation d'objects dans une image; indexage de documents)\n",
    "\n",
    "DL a été une grande avancée (avant = nécessitait des connaissances poussées niveau PhD pour étudier chacun d'entre eux) dans les techniques de reconnaissance de signaux de bas-niveau (correspond ~ modalités sensorielles), en permettant l'apprentissage des représentations intermédiaires.  \n",
    "En addition, le DL a fortement augmenté la puissance des modèles de reconnaissance et a permi de travailler sur ceux-ci sans nécessairement être un spécialiste du domaine modélisé.\n",
    "\n",
    "Le **neurone formel** :\n",
    "\n",
    "    y = f((wi^T)*xi+b)\n",
    "    \n",
    "       xi = inputs 1 -> i\n",
    "       wi, b = poids 1 -> i\n",
    "       f = activation function\n",
    "       y = output du neurone\n",
    "       \n",
    "L'apprentissage supervisé consiste en l'application d'un ensemble de paires annotées (xi,y\\*i) pendant la phase d'entraînement.\n",
    "\n",
    "    ŷi correspond à l'output prédit\n",
    "    y*i correspond à l'output réel\n",
    "    \n",
    "L'objectif de l'apprentissage supervisé est de minimiser une **fonction d'objectif** (correspond globalement à la différence entre ŷi et y\\*i ou à l'entropie croisée) et d'obtenir un modèle parametré :\n",
    "\n",
    "    fw(xi) = ŷi\n",
    "    \n",
    "Un neurone simple ne pourra réaliser que des prédictions (~projections) scalaires (ex. classification binaire).  \n",
    "Un réseau de neurones pourra réaliser des prédictions vectorielles (ex. régression logique).\n",
    "\n",
    "Le **perceptron multi-couche** (MLP) :\n",
    "\n",
    "Système basique du DL, le MLP consiste en l'accumulation de couches de réseaux de neurones pour complexifier le comportement de l'ensemble (peut approximer n'importe quelle fonction avec un nombre de neurones/couches suffisants). Considéré comme du DL dès la présence d'une couche cachée (donc dès 3 couches en tout, avec celles d'entrée et de sortie).\n",
    "\n",
    "---\n",
    "Pour la suite des exercices du cours, je dois installer [Keras](https://keras.io/), qui lui-même nécessite d'installer [TensorFlow](https://www.tensorflow.org/install/install_linux).  \n",
    "\n",
    "TensorFlow peut tourner sur GPU (CPU+GPU au lieu de CPU seul) lui permettant d'augmenter fortement sa vitesser de calcul, mais pour ça il faut posséder une carte graphique NVIDIA pouvant faire tourner [CUDA](http://docs.nvidia.com/cuda/cuda-installation-guide-linux/#axzz4VZnqTJ2A) (un toolkit de développement/programmation de NVIDIA).  \n",
    "C'est le cas sur ma machine physique, mais je programme sous une machine virtuelle (VirtualBox + Ubuntu) qui n'a pas nativement accès à mon GPU physique ([lien](https://devtalk.nvidia.com/default/topic/384440/cuda-programming-and-performance/cuda-on-a-virtual-machine/) et [lien](https://askubuntu.com/questions/598317/enable-cuda-gpu-working-under-vm)). Il serait possible de passer outre cette limitation via un *PCI Passthrough* ([lien](https://stackoverflow.com/questions/31627565/is-it-possible-to-develop-a-cuda-program-in-a-virtual-machine-that-has-a-ubuntu) et [lien](https://www.ibm.com/developerworks/library/l-pci-passthrough/). Ca peut valoir le coup de le mettre en place, mais la démarche est longue, technique et expérimentale; je la met de côté pour le moment.\n",
    "\n",
    "TensorFlow possède plusieurs moyens d'installation, j'ai choisi de passer par PyPI\n",
    "\n",
    "    sudo pip3 install tensorflow\n",
    "    \n",
    "J'ai ensuite pu vérifier l'intégrité de l'installation en faisant tourner un bloc basique de TF (selon [ces instructions](https://www.tensorflow.org/install/install_linux#ValidateYourInstallation)) :\n",
    "\n",
    "    python3\n",
    "    import tensorflow as tf\n",
    "    hello = tf.constant('Hello, TensorFlow!')\n",
    "    sess = tf.Session()\n",
    "    print(sess.run(hello))\n",
    "    > Hello, TensorFlow!\n",
    "    \n",
    "Puis j'ai pu installer Keras :\n",
    "\n",
    "    sudo pip3 install keras\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2017-12-14 - Cours Coursera\n",
    "---\n",
    "Exemple de modèle machine-learning simple pour la classification d'images sur mes notes manuscrites (page 7 et 8).\n",
    "\n",
    "Etant donné les deadlines, j'ai commencé le [cours de Coursera](https://www.coursera.org/learn/machine-learning/lecture/Ujm7v/what-is-machine-learning) en parallèle. Le cours est réalisé en Matlab/Octave (version opensource de Matlab), il faudra donc que j'installe Octave au moins pour le cours. Andrew Ng qui propose le cours décrit Octave comme le langage de programmation optimal pour faire du machine learning.  \n",
    "La traduction semble possible entre Octave et Python, mais Octave permet de passer un certain nombre d'étapes (fourni d'office certaines formules/algorithmes mathématiques). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2017-12-15\n",
    "\n",
    "Peu de notes hier et aujourd'hui, je suis en train de suivre le cours [Machine Learning](https://www.coursera.org/learn/machine-learning/) de Coursera.  \n",
    "Pour suivre les travaux pratiques, je dois installer Octave (nb. *Coursera possède un partenariat avec MatLab permettant de l'utiliser gratuitement pendant la durée du cours, ça peut être intéressant pour quelqu'un qui voudrait se familiariser avec cet environnement, mais je préfère l'option opensource).\n",
    "\n",
    "    sudo apt-get update\n",
    "    sudo apt-get install octave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# To Do\n",
    "+ ~~Je dois me renseigner sur les *mountain car models* et sur ZcA (*whitening decorrelation method* correspondant à un pré-traitement de l'image avant de la passer dans les *wavelets* et le modèle)~~\n",
    "+ Je dois me renseigner sur Mendeley pour gérer efficacement ma biblio, et la partager en ligne avec Daucé et Perrinet\n",
    "+ Je dois m'inscrire sur la liste de diffusion privée de l'INS\n",
    "+ ~~Je dois me renseigner sur TensorFlow~~\n",
    "+ ~~Je dois jeter un oeil à [Keras](https://keras.io/), une API python dédiée au machine learning et à la modélisation des réseaux de neurones, fonctionnant au dessus de TensorFlow, CNTK et Theano~~\n",
    "+ ~~Se renseigner sur cuDNN, permettant à Keras de tourner sur les GPU~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# A lire\n",
    "+ https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3361132/\n",
    "+ http://bethgelab.org/media/publications/Kuemmerer_High_Low_Level_Fixations_ICCV_2017.pdf\n",
    "+ https://pdfs.semanticscholar.org/0182/5573781674bcf85d0f5d2ec456842f75ad3c.pdf\n",
    "+ Schmidhuber, 1991 (voir mail Daucé)\n",
    "+ Parr and Friston, 2017 (voir mail Perrinet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
