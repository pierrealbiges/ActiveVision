{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2017-12-11 - Apprendre le machine learning\n",
    "---\n",
    "Cet après-midi 14h : rencontre avec Daucé (modèle + machine learning + tensorflow)\n",
    "\n",
    "Butko,2010 émet deux points importants, qui ne sont pas pris en compte dans son modèle : l'incertitude (à propos de l'environnement, de ses propres mouvements et de sa perception) et l'existance de plusieurs types de mouvements oculaires (pas seulement les saccades).\n",
    "\n",
    "TensorFlow est une libraire (majoritairement python, d'autres *API* sont disponibles : Java, C++ et Go) opensource développée par Google offrant une interface pour modéliser des réseaux de neurones artificiels et réaliser du *machine learning*.  \n",
    "Leur [site officiel](https://www.tensorflow.org/) montre qu'elle est utilisée par de nombreuses entreprises et contient une grande quantité de [documentation](https://www.tensorflow.org/get_started/).  \n",
    "L'ensemble du projet est présent sur leur [GitHub](https://github.com/tensorflow/) et contient même un [site interactif](http://playground.tensorflow.org) permettant de prendre en main son fonctionnement.  \n",
    "\n",
    "Suite de **ressources à maitriser** pour bosser sur le projet :\n",
    "+ [Cours CNAM machine learning/deep learning](http://cedric.cnam.fr/vertigo/Cours/ml2/preambule.html)\n",
    "+ [Cours Coursera machine learning](https://www.coursera.org/learn/machine-learning?action=enroll)\n",
    "+ [Cours Google deep learning](https://www.udacity.com/course/deep-learning--ud730)\n",
    "\n",
    "J'ai commencé à bosser la [première ressource](http://cedric.cnam.fr/vertigo/Cours/ml2/preambule.html) et je suis presque arrivé à la fin de la première partie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2017-12-12\n",
    "---\n",
    "\n",
    "## Introduction à l'apprentissage supervisé\n",
    "\n",
    "Pour évaluer la puissance d'un modèle, notamment son risque : méthode de l'**échantillon-test**. Division de la base d'apprentissage (D) en deux ensembles mutuellements exclusifs par sélection aléatoire (ex. D(100) = A(70)+V(30)). Réduit la taille de la base d'apprentissage (A) mais permet de calculer le risque espéré du modèle sur V (données n'ayant pas servi à l'apprentissage).  \n",
    "\n",
    "Plus robuste, la méthode de **validation croisée** propose de moyenner les résultats obtenus par plusieurs découpages différents de D.  \n",
    "Il existe plusieurs méthodes pour découper D (*Leave P out*, *Leave one out*, *k-fold*, *suffle and split*).  \n",
    "\n",
    "Evaluer la puissance d'un modèle passe aussi par la construction d'une **courbe ROC** permettant de montrer le taux de **vrais positifs**. Plus l'aire sous la courbe (*AUC*) est importante, plus le modèle est intéressant. \n",
    "\n",
    "---\n",
    "Plus d'informations sur le [mountain car problem](https://en.wikipedia.org/wiki/Mountain_car_problem) : problème communément appliqué aux modèles de *reinforcement learning* pour observer leur comportement et leur puissance.   \n",
    "L'agent comprends deux variables (position et vélocité) et trois actions (vers la gauche, neutre, vers la droite) et doit apprendre à les utiliser pour remonter une pente (qu'il ne peut pas simplement grimper, la puissance du moteur n'est pas assez importante) lui permettant d'obtenir une récompense (conséquence positive). Classiquement, chaque temps passé sans réaliser l'objectif (obtenir la récompense) est récompensé négativement (conséquence négative).  \n",
    "[Exemple d'application](https://github.com/vikasjiitk/Deep-RL-Mountain-Car)  \n",
    "\n",
    "Plus d'informations sur le [*whitening*](http://ufldl.stanford.edu/wiki/index.php/Whitening) : méthode de pré-traitement des données permettant de réduire leur dimension (en les rendant moins redondantes).  \n",
    "\n",
    "Il peut être intéressant de jeter un oeil à [ce livre](http://www.deeplearningbook.org/) disponible entièrement sur le net et écrit par des *deep-learning researchers* du MIT.\n",
    "\n",
    "---\n",
    "\n",
    "## Arbres de décision\n",
    "\n",
    "Les arbres de décision sont des graphes utilisant une représentation hiérarchique des données et comprenant trois types de noeuds :\n",
    "+ **Noeud racine** (d'où se fait l'accès à l'arbre)\n",
    "+ **Noeuds internes** (noeuds ayant des descendants et réalisant les **tests**)\n",
    "+ **Noeuds terminaux** (ou feuilles; qui n'ont pas de descendants et représentent les **décisions**)  \n",
    "\n",
    "Un élément de la base d’apprentissage situé dans un noeud ne pourra se retrouvera que dans un seul de ses descendants.  \n",
    "\n",
    "Au départ, les points des la base d’apprentissage sont tous placés dans le noeud racine.  \n",
    "\n",
    "Une des variables de description des points est la classe du point, dite « **variable cible** » (catégorielle dans un problème de classification, réelle dans un problème de régression).  \n",
    "\n",
    "Le processus s’arrête quand les éléments d’un noeud ont la même valeur pour la variable cible (homogénéité).\n",
    "\n",
    "Plusieurs types d'implantation : \n",
    "+ Iterative Dichotomiser 3 (ID3) et ses extensions ID4, ID5\n",
    "+ CART\n",
    "+ Bagging decision trees\n",
    "+ Forêts d'arbres décisionnels\n",
    "\n",
    "## Support vector machine (SVM)\n",
    "\n",
    "Base d'apprentissage {(xi,yi)} (i=1,...,n) avec :  \n",
    "\n",
    "    xi ∈ X  \n",
    "    yi ∈ {−1,+1}  \n",
    "\n",
    "Construction d'une fonction f:X→R tel que si f(x)>0 alors x est affecté à la classe +1 et si f(x)<0 alors x est affecté à la classe -1.   \n",
    "La ligne séparatrice (ou **hyperplan**) sera alors :\n",
    "\n",
    "    f(x)=0.  \n",
    "\n",
    "Pour juger la qualité d’un hyperplan en tant que séparateur on utilise la distance (marge) entre les exemples d’apprentissage et ce séparateur.   \n",
    "Les SVM linéaires cherchent l'hyperplan qui maximise la **marge** :\n",
    "\n",
    "    f(x) = ⟨w,x⟩+b = w(⊤)x+b\n",
    "\n",
    "    Si yi=1 alors w⋅xi+b≥1 et donc yi(w⋅xi+b)≥1\n",
    "    Si yi=−1 alors w⋅xi+b≤−1 et donc yi(w⋅xi+b)≥1\n",
    "  \n",
    "    Marge = 2/||w||\n",
    "    \n",
    "Souvent il arrive que les deux classes de données se retrouvent mélangées autours de l’hyperplan de séparation (donc non séparables linéairements). Pour gérer ce type de problème, on utilise une technique dite de **marge souple**, qui tolère les mauvais classements (compromis).  \n",
    "Dans cette technique, on introduit des **variables de relachement des contraintes ξi** (Absence d'erreur  ξi=0; Présence d'erreur ξi>0) qui permettent de modéliser les erreur potentielles (variables du mauvais côté de l'hyperplan) et de les pénaliser.  \n",
    "\n",
    "On introduit une **variable d’équilibrage C** (>0) qui permet d’avoir une seule fonction objectif :\n",
    "\n",
    "    min(w,b)(1/2*||w||²)+C*∑ξi\n",
    "    \n",
    "    \n",
    "Librairies majeures : **LibLinear**, **LibSVM**, **SVM-toy** et **Scikit-learn**\n",
    "L’intérêt de LibLinear est la vitesse, permettant de l’appliquer sur des grands jeux de données, mais ne peut l'être qu'à des problèmes linéaires.  \n",
    "LibSVM semble être plus puissant LibLinear (précision plus importante de la prédiction).  \n",
    "SVM-toi permet la visualisation de la classification pour des problèmes en deux dimensions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# To Do\n",
    "+ ~~Je dois me renseigner sur les *mountain car models* et sur ZcA (*whitening decorrelation method* correspondant à un pré-traitement de l'image avant de la passer dans les *wavelets* et le modèle)~~\n",
    "+ Je dois me renseigner sur Mendeley pour gérer efficacement ma biblio, et la partager en ligne avec Daucé et Perrinet\n",
    "+ Je dois m'inscrire sur la liste de diffusion privée de l'INS\n",
    "+ ~~Je dois me renseigner sur TensorFlow~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# A lire\n",
    "+ https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3361132/\n",
    "+ http://bethgelab.org/media/publications/Kuemmerer_High_Low_Level_Fixations_ICCV_2017.pdf\n",
    "+ https://pdfs.semanticscholar.org/0182/5573781674bcf85d0f5d2ec456842f75ad3c.pdf\n",
    "+ Schmidhuber, 1991 (voir mail Daucé)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
