{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T11:45:28.297040Z",
     "start_time": "2018-05-23T11:45:27.761162Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import noise\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import MotionClouds as mc\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from LogGabor import LogGabor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger la matrice de certitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T11:45:28.316512Z",
     "start_time": "2018-05-23T11:45:28.302099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading accuracy...\n",
      "[[0.0974 0.0974 0.0974 ... 0.0974 0.0974 0.0974]\n",
      " [0.0974 0.0974 0.0974 ... 0.0974 0.0974 0.0974]\n",
      " [0.0974 0.0974 0.0974 ... 0.0974 0.0974 0.0974]\n",
      " ...\n",
      " [0.0974 0.0974 0.0974 ... 0.0974 0.0974 0.0974]\n",
      " [0.0974 0.0974 0.0974 ... 0.0974 0.0974 0.0974]\n",
      " [0.0974 0.0974 0.0974 ... 0.0974 0.0974 0.0974]]\n"
     ]
    }
   ],
   "source": [
    "path = \"MNIST_accuracy.npy\"\n",
    "if os.path.isfile(path):\n",
    "    print('Loading accuracy...')\n",
    "    accuracy =  np.load(path)\n",
    "    print(accuracy)\n",
    "else:\n",
    "    print('No accuracy data found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparer l'apprentissage et les fonctions nécessaires au fonctionnement du script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T11:45:33.948752Z",
     "start_time": "2018-05-23T11:45:28.319226Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorization(N_theta, N_orient, N_scale, N_phase, N_X, N_Y, rho):\n",
    "    phi = np.zeros((N_theta, N_orient, N_scale, N_phase, N_X*N_Y))\n",
    "    parameterfile = 'https://raw.githubusercontent.com/bicv/LogGabor/master/default_param.py'\n",
    "    lg = LogGabor(parameterfile)\n",
    "    lg.set_size((N_X, N_Y))\n",
    "    params = {'sf_0': .1, 'B_sf': lg.pe.B_sf,\n",
    "              'theta': np.pi * 5 / 7., 'B_theta': lg.pe.B_theta}\n",
    "    phase = np.pi/4\n",
    "    edge = lg.normalize(lg.invert(lg.loggabor(\n",
    "        N_X/3, 3*N_Y/4, **params)*np.exp(-1j*phase)))\n",
    "\n",
    "    for i_theta in range(N_theta):\n",
    "        for i_orient in range(N_orient):\n",
    "            for i_scale in range(N_scale):\n",
    "                ecc = (1/rho)**(N_scale - i_scale)\n",
    "                r = np.sqrt(N_X**2+N_Y**2) / 2 * ecc  # radius\n",
    "                sf_0 = 0.5 * 0.03 / ecc\n",
    "                x = N_X/2 + r * \\\n",
    "                    np.cos((i_orient+(i_scale % 2)*.5)*np.pi*2 / N_orient)\n",
    "                y = N_Y/2 + r * \\\n",
    "                    np.sin((i_orient+(i_scale % 2)*.5)*np.pi*2 / N_orient)\n",
    "                for i_phase in range(N_phase):\n",
    "                    params = {'sf_0': sf_0, 'B_sf': lg.pe.B_sf,\n",
    "                              'theta': i_theta*np.pi/N_theta, 'B_theta': np.pi/N_theta/2}\n",
    "                    phase = i_phase * np.pi/2\n",
    "                    phi[i_theta, i_orient, i_scale, i_phase, :] = lg.normalize(\n",
    "                        lg.invert(lg.loggabor(x, y, **params)*np.exp(-1j*phase))).ravel()\n",
    "    return phi\n",
    "\n",
    "\n",
    "N_theta, N_orient, N_scale, N_phase, N_X, N_Y, rho = 6, 12, 5, 2, 128, 128, 1.95\n",
    "phi = vectorization(N_theta, N_orient, N_scale, N_phase, N_X, N_Y, rho)\n",
    "phi_vector = phi.reshape((N_theta*N_orient*N_scale*N_phase, N_X*N_Y))\n",
    "phi_plus = np.linalg.pinv(phi_vector)\n",
    "\n",
    "energy = (phi**2).sum(axis=(0, 3))\n",
    "energy /= energy.sum(axis=-1)[:, :, None]\n",
    "energy_vector = energy.reshape((N_orient*N_scale, N_X*N_Y))\n",
    "energy_plus = np.linalg.pinv(energy_vector)\n",
    "\n",
    "\n",
    "def accuracy_128(i_offset, j_offset, N_pic=128, N_stim=55):\n",
    "    center = (N_pic-N_stim)//2\n",
    "\n",
    "    accuracy_128 = 0.1 * np.ones((N_pic, N_pic))\n",
    "    accuracy_128[(center+i_offset):(center+N_stim+i_offset),\n",
    "                 (center+j_offset):(center+N_stim+j_offset)] = accuracy\n",
    "\n",
    "    accuracy_LP = energy_vector @ np.ravel(accuracy_128)\n",
    "    return accuracy_LP\n",
    "\n",
    "\n",
    "def mnist_128(data, i_offset, j_offset, N_pic=128, N_stim=28, noise=True, noise_type='MotionCloud'):\n",
    "    center = (N_pic-N_stim)//2\n",
    "\n",
    "    #data_128 = np.zeros((N_pic, N_pic))\n",
    "    data_128 = (data.min().numpy()) * np.ones((N_pic, N_pic))\n",
    "\n",
    "\n",
    "    data_128[(center+i_offset):(center+N_stim+i_offset),\n",
    "             (center+j_offset):(center+N_stim+j_offset)] = data\n",
    "\n",
    "    if noise:\n",
    "        if noise_type == 'MotionCloud':\n",
    "            data_LP = phi_vector @ np.ravel(data_128 + MotionCloudNoise())\n",
    "        elif noise_type == 'Perlin':\n",
    "            data_LP = phi_vector @ np.ravel(\n",
    "                data_128 + randomized_perlin_noise())\n",
    "    else:\n",
    "        data_LP = phi_vector @ np.ravel(data_128)\n",
    "    return data_LP\n",
    "\n",
    "\n",
    "def couples(data, i_offset, j_offset):\n",
    "    v = mnist_128(data, i_offset, j_offset)\n",
    "    a = accuracy_128(i_offset, j_offset)\n",
    "    return (v, a)\n",
    "\n",
    "\n",
    "def minmax(value, border):\n",
    "    value = max(value, -border)\n",
    "    value = min(value, border)\n",
    "    return value\n",
    "\n",
    "\n",
    "def sigmoid(values):\n",
    "    values = 1 / (1 + ((1 / 0.1) - 1) * np.exp(-values))\n",
    "    return values\n",
    "\n",
    "\n",
    "def randomized_perlin_noise(shape=(128, 128), scale=10, octaves=6, persistence=0.5, lacunarity=2.0, base=0):\n",
    "    noise_vector = np.zeros(shape)\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            noise_vector[i][j] = noise.pnoise2(i/scale,\n",
    "                                               j/scale,\n",
    "                                               octaves=int(\n",
    "                                                   octave * abs(np.random.randn()))+1,\n",
    "                                               persistence=persistence *\n",
    "                                               abs(np.random.randn()),\n",
    "                                               lacunarity=lacunarity *\n",
    "                                               abs(np.random.randn()),\n",
    "                                               repeatx=shape[0],\n",
    "                                               repeaty=shape[1],\n",
    "                                               base=base)\n",
    "    return noise_vector\n",
    "\n",
    "\n",
    "def MotionCloudNoise(sf_0=0.125, B_sf=3.):\n",
    "    mc.N_X, mc.N_Y, mc.N_frame = 128, 128, 1\n",
    "    fx, fy, ft = mc.get_grids(mc.N_X, mc.N_Y, mc.N_frame)\n",
    "    name = 'static'\n",
    "    env = mc.envelope_gabor(fx, fy, ft, sf_0=sf_0, B_sf=B_sf,\n",
    "                            B_theta=np.inf, V_X=0., V_Y=0., B_V=0, alpha=.5)\n",
    "\n",
    "    z = mc.rectif(mc.random_cloud(env))\n",
    "    z = z.reshape((mc.N_X, mc.N_Y))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réseau de neurones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T11:45:34.019533Z",
     "start_time": "2018-05-23T11:45:33.958869Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_size = 100 #quantity of examples that'll be processed\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('/tmp/data', \n",
    "                   train=True,    #def the dataset as training data \n",
    "                   download=True, #download if dataset not present on disk\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                   batch_size=sample_size, \n",
    "                   shuffle=True)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden1, n_RNN, n_hidden2, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(n_feature, n_hidden1)\n",
    "        self.RNN = torch.nn.RNN(n_hidden1, n_RNN, nonlinearity='relu', bias=True, bidirectional=False)\n",
    "        self.hidden2 = torch.nn.Linear(n_RNN, n_hidden2)\n",
    "        self.predict = torch.nn.Linear(n_hidden2, n_output)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        data = F.leaky_relu(self.hidden1(data))\n",
    "        data, weights = self.RNN(data)\n",
    "        data = F.leaky_relu(self.hidden2(data))\n",
    "        data = self.predict(data)\n",
    "        data = F.sigmoid(data)\n",
    "        return data\n",
    "    \n",
    "net = Net(n_feature=720, n_hidden1=585, n_RNN=390, n_hidden2=195, n_output=60)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.3)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "def train(sample_size, vsize=720, asize=60):\n",
    "    t_start = time.time()\n",
    "    print('Starting training...')\n",
    "    for batch_idx, (data, label) in enumerate(data_loader):\n",
    "        input, target = np.zeros((sample_size, 1, vsize)), np.zeros((sample_size, 1, asize))\n",
    "        for idx in range(sample_size):\n",
    "            i_offset, j_offset = int(minmax(np.random.randn()*5, 35)), int(minmax(np.random.randn()*5, 35))\n",
    "            input[idx,0,:], target[idx,0,:] = couples(data[idx,0,:], i_offset, j_offset)\n",
    "\n",
    "        input, target = Variable(torch.FloatTensor(input)), Variable(torch.FloatTensor(target))\n",
    "\n",
    "        prediction = net(input)\n",
    "        loss = loss_func(prediction, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch {}: [{}/{}] Loss: {} Time: {:.2f} mn'.format(\n",
    "                epoch, batch_idx*sample_size, len(data_loader.dataset), \n",
    "                loss.data.numpy(), (time.time()-t_start)/60))\n",
    "    \n",
    "def eval_sacc(vsize=720, asize=60, N_pic=128, sacc_lim=10, fovea_size=10, fig_type='cmap'):\n",
    "    for batch_idx, (data, label) in enumerate(data_loader):\n",
    "        i_offset, j_offset = int(minmax(np.random.randn()*10, 35)), int(minmax(np.random.randn()*10, 35))\n",
    "        print('Stimulus position: ({},{})'.format(i_offset, j_offset))\n",
    "        target_in_fovea = False\n",
    "        sacc_count = 0\n",
    "        \n",
    "        while not target_in_fovea:\n",
    "            input, target = np.zeros((1, 1, vsize)), np.zeros((1, 1, asize))\n",
    "            input[0,0,:], target[0,0,:] = couples(data[0,0,:], i_offset, j_offset)\n",
    "            input, target = Variable(torch.FloatTensor(input)), Variable(torch.FloatTensor(target))\n",
    "\n",
    "            prediction = net(input)        \n",
    "            pred_data = prediction.data.numpy()[-1][-1]\n",
    "\n",
    "            if fig_type == 'cmap':\n",
    "                image = sigmoid(energy_plus @ pred_data)\n",
    "                image_reshaped = image.reshape(N_pic,N_pic)\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(13,10.725))\n",
    "                cmap = ax.pcolor(np.arange(-(N_pic/2),(N_pic/2)), np.arange(-(N_pic/2),(N_pic/2)), image_reshaped)\n",
    "                fig.colorbar(cmap)\n",
    "                plt.axvline(j_offset, c='k'); plt.axhline(i_offset, c='k')\n",
    "                \n",
    "                for i_pred in range(0,N_pic):\n",
    "                    for j_pred in range(0,N_pic):\n",
    "                        if image_reshaped[i_pred][j_pred] == image_reshaped.max():\n",
    "                            i_hat, j_hat = i_pred-(N_pic/2), j_pred-(N_pic/2)\n",
    "                            print('Position prediction: ({},{})'.format(i_hat, j_hat))\n",
    "                            if fig_type == 'cmap':\n",
    "                                plt.axvline(j_hat, c='r'); plt.axhline(i_hat, c='r')\n",
    "                            break\n",
    "\n",
    "                #check if number of saccades is beyond threshold   \n",
    "                if sacc_count == sacc_lim:\n",
    "                    print('Stimulus position not found, break')\n",
    "                    break\n",
    "\n",
    "                #saccades\n",
    "                i_offset, j_offset = (i_offset - i_hat), (j_offset - j_hat)\n",
    "                sacc_count += 1\n",
    "                print('Stimulus position after saccade: ({}, {})'.format(i_offset, j_offset))\n",
    "                \n",
    "                #check if the image position is predicted within the fovea\n",
    "                if i_hat <= (fovea_size/2) and j_hat <= (fovea_size/2):\n",
    "                    if i_hat >= -(fovea_size/2) and j_hat >= -(fovea_size/2):\n",
    "                        target_in_fovea = True\n",
    "                        print('target predicted in fovea, stopping the saccadic exploration')\n",
    "            \n",
    "            if fig_type == 'log':\n",
    "                code = energy_plus @ np.ravel(pred_data)\n",
    "                code = phi @ code\n",
    "                global_energy = (code**2).sum(axis=(0, -1))\n",
    "                \n",
    "                log_r_target = 1 + np.log(np.sqrt(i_offset**2 + j_offset**2) / np.sqrt(N_X**2 + N_Y**2) / 2) / 5\n",
    "                if j_offset != 0:\n",
    "                    theta_target = np.arctan(-i_offset / j_offset)\n",
    "                else:\n",
    "                    theta_target = np.sign(-i_offset) * np.pi/2\n",
    "                print('target position (log_r, theta) = ({},{})'.format(log_r_target, theta_target))\n",
    "                log_r, theta = np.meshgrid(np.linspace(0, 1, N_scale+1), np.linspace(-np.pi*.625, np.pi*1.375, N_orient+1))\n",
    "\n",
    "                fig, ax = plt.subplots(subplot_kw=dict(projection='polar'))\n",
    "                ax.pcolor(theta, log_r, np.fliplr(global_energy))\n",
    "                ax.plot(theta_target, log_r_target, 'r+')\n",
    "                \n",
    "                for n_orient in range(N_orient):\n",
    "                    for n_scale in range(N_scale):\n",
    "                        if global_energy[n_orient][n_scale] == np.max(global_energy):\n",
    "                            print('Position prediction (orient, scale) = ({},{})'.format(n_orient, n_scale))\n",
    "                \n",
    "                target_in_fovea = True\n",
    "        \n",
    "        print('*' * 50)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancer l'apprentissage ou charger les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-23T11:45:27.750Z"
    }
   },
   "outputs": [],
   "source": [
    "path = 'regression_RNN_network.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l {path}\n",
    "#!rm {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-23T11:45:27.750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Starting training...\n",
      "Epoch 0: [0/60000] Loss: 0.16390405595302582 Time: 0.03 mn\n",
      "Epoch 0: [10000/60000] Loss: 0.009776144288480282 Time: 3.50 mn\n",
      "Epoch 0: [20000/60000] Loss: 0.006793758366256952 Time: 7.13 mn\n",
      "Epoch 0: [30000/60000] Loss: 0.0050132861360907555 Time: 10.73 mn\n",
      "Epoch 0: [40000/60000] Loss: 0.004601474851369858 Time: 14.25 mn\n",
      "Epoch 0: [50000/60000] Loss: 0.004544503055512905 Time: 17.81 mn\n",
      "Model saved at regression_RNN_network.pt\n",
      "Starting training...\n",
      "Epoch 1: [0/60000] Loss: 0.003965731710195541 Time: 0.03 mn\n",
      "Epoch 1: [10000/60000] Loss: 0.0032039168290793896 Time: 3.61 mn\n",
      "Epoch 1: [20000/60000] Loss: 0.0033648768439888954 Time: 7.21 mn\n",
      "Epoch 1: [30000/60000] Loss: 0.0031828342471271753 Time: 10.80 mn\n",
      "Epoch 1: [40000/60000] Loss: 0.002684887032955885 Time: 14.36 mn\n",
      "Epoch 1: [50000/60000] Loss: 0.0026857820339500904 Time: 18.03 mn\n",
      "Model saved at regression_RNN_network.pt\n",
      "Starting training...\n",
      "Epoch 2: [0/60000] Loss: 0.0023167321924120188 Time: 0.03 mn\n",
      "Epoch 2: [10000/60000] Loss: 0.002555835759267211 Time: 3.62 mn\n",
      "Epoch 2: [20000/60000] Loss: 0.002620887476950884 Time: 7.25 mn\n",
      "Epoch 2: [30000/60000] Loss: 0.002650264650583267 Time: 11.16 mn\n",
      "Epoch 2: [40000/60000] Loss: 0.0023868319112807512 Time: 15.47 mn\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(path):\n",
    "    net.load_state_dict(torch.load(path))\n",
    "    print('Loading file', path)\n",
    "else:\n",
    "    print('Training model...')\n",
    "    N_epochs = 10\n",
    "    for epoch in range(N_epochs):                 #max number of training epochs\n",
    "        train(sample_size)                 #starting the learning\n",
    "        torch.save(net.state_dict(), path) #save the neural network state\n",
    "        print('Model saved at', path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancer l'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-23T11:45:27.751Z"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    eval_sacc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-23T11:45:27.753Z"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    eval_sacc(fig_type='log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
